{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from config import args_dict\n",
    "from model import T5NerFineTuner\n",
    "from dataset import T5NerFineTunerDataModule\n",
    "\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setproctitle import setproctitle\n",
    "from utils.set_seed import set_seed\n",
    "\n",
    "setproctitle(\"taejung_t2t_ner_evaluate\")\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_dir': '', 'lightning_model_checkpoint': '', 'model_name_or_path': '/home/work/team01/ICU_models/template-ULM_Y/kt-ulm-small-cross01/checkpoint-33000', 'tokenizer_name_or_path': '/home/work/team01/ICU_models/template-ULM_Y/kt-ulm-small-cross01/checkpoint-33000', 'cached_dataset_path': 'cached_dataset', 'max_seq_length': 256, 'learning_rate': 0.0003, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'batch_size': 8, 'max_epochs': 15, 'gradient_accumulation_steps': 16, 'max_grad_norm': 1.0, 'seed': 42, 'accelerator': 'gpu', 'devices': 1}\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"/home/work/team01/ICU_models/template-ULM_Y/kt-ulm-small-cross01/checkpoint-33000\"\n",
    "args_dict.update(\n",
    "    dict(\n",
    "        # lightning_model_checkpoint=\"/home/work/team01/ICU_models/template-ner_T/checkpoints/epoch=5-step=37446.ckpt\",\n",
    "        model_name_or_path=checkpoint,\n",
    "        tokenizer_name_or_path=checkpoint,\n",
    "        cached_dataset_path=\"cached_dataset\",\n",
    "    )\n",
    ")\n",
    "print(args_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/work/.conda/envs/htj/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(**args_dict)\n",
    "\n",
    "trainer = pl.Trainer.from_argparse_args(args)\n",
    "datamodule = T5NerFineTunerDataModule(**args_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-824f6f1c9fb5927a\n",
      "Found cached dataset csv (/home/work/.cache/huggingface/datasets/csv/default-824f6f1c9fb5927a/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f946999261545078eb7f5fb2d3fcf87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "캐시된 데이터셋 사용\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at cached_dataset/train/cache-f4cfe387481aeba5.arrow and cached_dataset/train/cache-80cbb823f257c523.arrow\n"
     ]
    }
   ],
   "source": [
    "datamodule.prepare_data()\n",
    "datamodule.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"accelerator\":                 gpu\n",
      "\"adam_epsilon\":                1e-08\n",
      "\"batch_size\":                  8\n",
      "\"cached_dataset_path\":         cached_dataset\n",
      "\"devices\":                     1\n",
      "\"gradient_accumulation_steps\": 16\n",
      "\"learning_rate\":               0.0003\n",
      "\"lightning_model_checkpoint\":  \n",
      "\"max_epochs\":                  15\n",
      "\"max_grad_norm\":               1.0\n",
      "\"max_seq_length\":              256\n",
      "\"model_name_or_path\":          /home/work/team01/ICU_models/template-ULM_Y/kt-ulm-small-cross01/checkpoint-33000\n",
      "\"output_dir\":                  \n",
      "\"seed\":                        42\n",
      "\"tokenizer_name_or_path\":      /home/work/team01/ICU_models/template-ULM_Y/kt-ulm-small-cross01/checkpoint-33000\n",
      "\"total_steps\":                 12482\n",
      "\"warmup_steps\":                0\n",
      "\"weight_decay\":                0.0\n"
     ]
    }
   ],
   "source": [
    "total_steps = len(datamodule.train_dataloader())\n",
    "\n",
    "model = None\n",
    "if args.lightning_model_checkpoint:\n",
    "    print(\"Load from checkpoint\")\n",
    "    model = T5NerFineTuner.load_from_checkpoint(args.lightning_model_checkpoint)\n",
    "else:\n",
    "    model = T5NerFineTuner(total_steps=total_steps, **args_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/work/.conda/envs/htj/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1523c20af3ec4d28a9191717af7492b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = trainer.predict(model, dataloaders=datamodule.test_dataloader())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/.conda/envs/htj/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1277c02c9d664f6dac36048bf7c4d8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3901 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from transformers import T5TokenizerFast\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(args.tokenizer_name_or_path, fast=True)\n",
    "\n",
    "predicted_tags = []\n",
    "for i, r in enumerate(tqdm(results)):\n",
    "    decoded_strings = tokenizer.decode(r, skip_special_tokens=True)\n",
    "    predicted_tags.extend(\n",
    "        list(\n",
    "            map(\n",
    "                lambda x: x.strip(),\n",
    "                filter(lambda x: x != \"\", decoded_strings.split(\"[label] \")),\n",
    "            )\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bced3bb1d53646eca71843407abdd137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3901 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "test_dataloader = datamodule.test_dataloader()\n",
    "\n",
    "for i, r in enumerate(tqdm(test_dataloader)):\n",
    "    batch_size = len(r[\"input_ids\"])\n",
    "    sentences.extend(\n",
    "        [tokenizer.decode(t, skip_special_tokens=True) for t in r[\"input_ids\"]]\n",
    "    )\n",
    "    labels.extend([tokenizer.decode(t, skip_special_tokens=True) for t in r[\"labels\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove [sentence], [label] indicators\n",
    "sentence_start = len(\"[sentence]\")\n",
    "label_start = len(\"[label]\")\n",
    "sentences = list(map(lambda x: x[sentence_start:].strip(), sentences))\n",
    "labels = list(map(lambda x: x[label_start:].strip(), labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [1, -1, -2]:\n",
    "#     print(\"tags:\", tags[i])\n",
    "#     print(\"labels:\", labels[i])\n",
    "#     print(\"sentences:\", sentences[i])\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_6_tags = []\n",
    "split_6_sentences = []\n",
    "split_6_labels = []\n",
    "for i in range(len(predicted_tags)):\n",
    "    if i % 6 == 0:\n",
    "        split_6_tags.append([])\n",
    "        split_6_sentences.append([])\n",
    "        split_6_labels.append([])\n",
    "    split_6_tags[-1].append(predicted_tags[i])\n",
    "    split_6_sentences[-1].append(sentences[i])\n",
    "    split_6_labels[-1].append(labels[i])\n",
    "\n",
    "# print(split_6_tags[0])\n",
    "# print(split_6_labels[0])\n",
    "# print(split_6_sentences[0])\n",
    "# print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_and_remove_last_comma(text):\n",
    "    text = text.strip()\n",
    "    if text.endswith(\",\"):\n",
    "        text = text[:-1]\n",
    "    return text\n",
    "\n",
    "\n",
    "def compute_f1_score(predicted_tags, labels, sentences, debug=False):\n",
    "    def debug_print(*args, **kwargs):\n",
    "        if debug:\n",
    "            print(*args, **kwargs)\n",
    "\n",
    "    def tqdm_zip(*args):\n",
    "        zipped = zip(*args)\n",
    "        if debug:\n",
    "            return zipped\n",
    "        return tqdm(zipped)\n",
    "\n",
    "    ner_tags = [\"QT\", \"PS\", \"LC\", \"DT\", \"TI\", \"OG\"]\n",
    "    tp_fp_fn = []\n",
    "    for current_predicted_tags, current_labels, current_sentences in tqdm_zip(\n",
    "        predicted_tags, labels, sentences\n",
    "    ):\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "        # 한 문장에 대한 f1 스코어 계산\n",
    "        for idx, (ner_tag, predicted, label) in enumerate(\n",
    "            zip(ner_tags, current_predicted_tags, current_labels)\n",
    "        ):\n",
    "            p_words = list(\n",
    "                map(strip_and_remove_last_comma, predicted.split(f\"{ner_tag}:\"))\n",
    "            )\n",
    "            l_words = list(map(strip_and_remove_last_comma, label.split(f\"{ner_tag}:\")))\n",
    "            debug_print(\"ner_tag:\", ner_tag, \"p_words:\", p_words)\n",
    "            debug_print(\"ner_tag:\", ner_tag, \"l_words:\", l_words)\n",
    "\n",
    "            # entity가 없는데 있다고 한 경우\n",
    "            if l_words[0] == \"O\" and p_words[0] != \"O\":\n",
    "                debug_print(\"0개, N개\", end=\" \")\n",
    "                debug_print(\"fp:\", fp, end=\" -> \")\n",
    "                fp += len(p_words[1:])\n",
    "                debug_print(fp)\n",
    "                continue\n",
    "\n",
    "            # entity가 있는데 없다고 한 경우\n",
    "            elif l_words[0] != \"O\" and p_words[0] == \"O\":\n",
    "                debug_print(\"N개, 0개\", end=\" \")\n",
    "                debug_print(\"fn:\", fn, end=\" -> \")\n",
    "                fn += len(l_words[1:])\n",
    "                debug_print(fn)\n",
    "                continue\n",
    "\n",
    "            # 둘 다 O인 경우에는 패스\n",
    "            elif l_words[0] == \"O\" and p_words[0] == \"O\":\n",
    "                debug_print(\"0개, 0개 pass~\")\n",
    "                continue\n",
    "\n",
    "            # 둘 다 O가 아니므로 첫 칸은 빈칸 -> 제거\n",
    "            p_words = p_words[1:]\n",
    "            l_words = l_words[1:]\n",
    "\n",
    "            # TP 계산\n",
    "            temp_l_words = l_words[:]\n",
    "            temp_p_words = p_words[:]\n",
    "            for l_word in temp_l_words:\n",
    "                find_word = False\n",
    "                for p_word in temp_p_words[:]:\n",
    "                    # label과 predicted가 일치한 경우\n",
    "                    # predicted words list에서 해당 아이템\n",
    "                    if l_word == p_word:\n",
    "                        debug_print(f\"<{l_word}>, <{p_word}>같음! tp:\", tp, end=\" -> \")\n",
    "                        tp += 1\n",
    "                        debug_print(tp)\n",
    "                        find_word = True\n",
    "\n",
    "                    # 예측: <서울시 강남구>, 정답: <서울시>인 경우\n",
    "                    # 예측된 단어가 정답으로 시작할 때\n",
    "                    #    -> 아닌 경우 <남서울시>\n",
    "                    elif p_word.startswith(l_word):\n",
    "                        debug_print(\n",
    "                            f\"<{l_word}> <{p_word}> p_word가 l_word로 시작!\", tp, end=\" -> \"\n",
    "                        )\n",
    "                        tp += 1\n",
    "                        debug_print(tp)\n",
    "                        find_word = True\n",
    "                        # === 이후 상황\n",
    "\n",
    "                        # 다음 l_word <강남구>로 이동\n",
    "                        # p_word가 지워졌기 때문에 다음 l_word인 강남구는\n",
    "                        # find_word = False로 자연스럽게 fn으로 처리됨\n",
    "\n",
    "                    # 예측: <서울시>, 정답: <서울시 강남구>인 경우\n",
    "                    # 정답이 예측된 단어로 시작될 때\n",
    "                    #    -> 아닌 경우 <서울시청>\n",
    "                    elif l_word.startswith(p_word):\n",
    "                        debug_print(\n",
    "                            f\"<{l_word}> <{p_word}> l_word가 p_word로 시작!\", tp, end=\" -> \"\n",
    "                        )\n",
    "                        tp += 1\n",
    "                        find_word = True\n",
    "\n",
    "                        # === 이후 상황\n",
    "\n",
    "                        # 다음 l_word <대치동>로 이동\n",
    "                        # 자연스럽게 다음 체크 과정 거침\n",
    "\n",
    "                        # 강남구는 마지막까지 temp_p_words에서 사라지지 않아서\n",
    "                        # named entity로 예측했지만 False인 FP로 처리됨\n",
    "\n",
    "                    # 사용한 단어는 제거\n",
    "                    # 다음 l_word 정답 체크\n",
    "                    if find_word:\n",
    "                        debug_print(f\"단어 찾았으니까 <{p_word}> 지움!\")\n",
    "                        temp_p_words.remove(p_word)\n",
    "                        break\n",
    "\n",
    "                # p_words를 다 체크했는데 word를 찾지 못한 경우\n",
    "                if not find_word:\n",
    "                    debug_print(f\"<{l_word}>가 predicted words에 없음! fn:\", fn, end=\" -> \")\n",
    "                    fn += 1\n",
    "                    debug_print(fn)\n",
    "\n",
    "            # 마지막까지 label과 매칭되지 못한 예측된 NE들은\n",
    "            # FP로 처리됨\n",
    "            debug_print(temp_p_words, \"가 label과 매칭되지 못했음 fp:\", fp, end=\" -> \")\n",
    "            fp += len(temp_p_words)\n",
    "            debug_print(fp)\n",
    "\n",
    "        debug_print(current_sentences[0].split(\"[tag]\")[0])\n",
    "        debug_print(\"tp:\", tp, \"fp:\", fp, \"fn:\", fn)\n",
    "        debug_print()\n",
    "        tp_fp_fn.append((tp, fp, fn))\n",
    "\n",
    "    tp = sum([tff[0] for tff in tp_fp_fn])\n",
    "    fp = sum([tff[1] for tff in tp_fp_fn])\n",
    "    fn = sum([tff[2] for tff in tp_fp_fn])\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e20a565a60b4d579f467876a72b54d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9107770779396421\n"
     ]
    }
   ],
   "source": [
    "results = compute_f1_score(split_6_tags, split_6_labels, split_6_sentences, debug=False)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "###################\n",
    "# Test cases\n",
    "###################\n",
    "# test_tags = [\n",
    "#     [\"O\", \"O\", \"LC:서울시,LC:강남구\", \"O\", \"O\", \"O\"],\n",
    "#     [\"O\", \"O\", \"LC:서울시 강남구\", \"O\", \"O\", \"O\"],\n",
    "#     [\"O\", \"O\", \"LC:서울시,LC:강남구\", \"O\", \"O\", \"O\"],\n",
    "#     [\"O\", \"PS:허태정\", \"O\", \"O\", \"O\", \"O\"],\n",
    "#     [\"O\", \"PS:허태정\", \"O\", \"O\", \"O\", \"O\"],\n",
    "#     [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n",
    "#     [\"O\", \"PS:허태정\", \"O\", \"O\", \"O\", \"O\"],\n",
    "#     [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n",
    "# ]\n",
    "# test_labels = [\n",
    "#     [\"O\", \"O\", \"LC:서울시,LC:강남구\", \"O\", \"O\", \"O\"],\n",
    "#     [\"O\", \"O\", \"LC:서울시,LC:강남구\", \"O\", \"O\", \"O\"],\n",
    "#     [\"O\", \"O\", \"LC:서울시 강남구\", \"O\", \"O\", \"O\"],\n",
    "#     [\"O\", \"PS:감자튀김\", \"O\", \"O\", \"O\", \"O\"],\n",
    "#     [\"O\", \"O\", \"LC:허태정\", \"O\", \"O\", \"O\"],\n",
    "#     [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n",
    "#     [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\"],\n",
    "#     [\"O\", \"PS:허태정\", \"O\", \"O\", \"O\", \"O\"],\n",
    "# ]\n",
    "# test_sentences = [[\"허태정\" + str(j) for j in range(6)] for i in range(len(test_labels))]\n",
    "# f1_scores = compute_f1_score(test_tags, test_labels, test_sentences, debug=True)\n",
    "# print(f1_scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('htj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78e8a12774b339fd8edd98deaf1aeb1922d7df36f652a582ef30e18967d80323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
